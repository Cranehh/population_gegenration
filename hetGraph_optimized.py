import torch
import torch.nn.functional as F
import torch.nn as nn
import math


def convert_graphvae_to_differentiable(decoder, use_gumbel_softmax=True, temperature=1.0, hard=False):
    """
    å°†GraphVAEçš„logitsè¾“å‡ºè½¬æ¢ä¸ºå¯å¾®åˆ†çš„è½¯å›¾ç»“æ„
    """
    adj_logits = decoder._tilde_structure.adj_matrices_special_diag
    edge_logits = decoder._tilde_structure.edge_atr_tensors
    node_logits = decoder._tilde_structure.node_atr_matrices

    if use_gumbel_softmax:
        adj_soft = torch.sigmoid(adj_logits)
        edge_soft = F.gumbel_softmax(edge_logits, tau=temperature, hard=hard, dim=-1)
        node_soft = F.gumbel_softmax(node_logits, tau=temperature, hard=hard, dim=-1)
    else:
        adj_soft = torch.sigmoid(adj_logits)
        edge_soft = F.softmax(edge_logits, dim=-1)
        node_soft = F.softmax(node_logits, dim=-1)

    # 1. æ‰¾å‡ºæœ‰æ•ˆè¿æ¥ï¼ˆadj > 0.8ï¼‰[batch, 8, 8]
    valid_edge_mask = (adj_soft > 0.8).float()

    # 2. æ‰¾å‡ºæœ‰æ•ˆèŠ‚ç‚¹ï¼ˆè¡Œå’Œ > 0.8ï¼‰[batch, 8]
    node_degree = adj_soft.sum(dim=2)  # [batch, 8]
    valid_node_mask = (node_degree > 0.8).float()  # [batch, 8]

    # 3. å¤„ç†æ²¡æœ‰æœ‰æ•ˆèŠ‚ç‚¹çš„æƒ…å†µï¼šä¿ç•™ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
    # æ£€æŸ¥æ¯ä¸ªbatchæ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆèŠ‚ç‚¹
    has_valid_nodes = valid_node_mask.sum(dim=1) > 0  # [batch]
    # å¯¹äºæ²¡æœ‰æœ‰æ•ˆèŠ‚ç‚¹çš„batchï¼Œå¼ºåˆ¶ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä¸ºæœ‰æ•ˆ
    valid_node_mask[:, 0] = torch.where(has_valid_nodes, valid_node_mask[:, 0], torch.ones_like(valid_node_mask[:, 0]))

    # 4. è¿‡æ»¤é‚»æ¥çŸ©é˜µï¼šåªä¿ç•™æœ‰æ•ˆèŠ‚ç‚¹ä¹‹é—´çš„æœ‰æ•ˆè¿æ¥
    # åˆ›å»ºèŠ‚ç‚¹æ©ç çš„å¹¿æ’­ç‰ˆæœ¬ [batch, 8, 1] å’Œ [batch, 1, 8]
    node_mask_i = valid_node_mask.unsqueeze(2)  # [batch, 8, 1]
    node_mask_j = valid_node_mask.unsqueeze(1)  # [batch, 1, 8]

    # # è¾¹å¿…é¡»è¿æ¥ä¸¤ä¸ªæœ‰æ•ˆèŠ‚ç‚¹ï¼Œä¸”è¾¹æƒé‡ > 0.8
    # valid_edge_mask = valid_edge_mask * node_mask_i * node_mask_j  # [batch, 8, 8]

    # 5. è¿‡æ»¤è¾¹ç‰¹å¾ï¼šä½¿ç”¨æ©ç åŠ æƒ
    # edge_soft: [batch, 8, 8, 5]
    # valid_edge_mask: [batch, 8, 8] -> [batch, 8, 8, 1]
    edge_mask_expanded = valid_edge_mask.unsqueeze(-1)  # [batch, 8, 8, 1]
    edge_filtered = edge_soft * edge_mask_expanded  # [batch, 8, 8, 5]

    # 6. è¿‡æ»¤èŠ‚ç‚¹ç‰¹å¾ï¼šä½¿ç”¨æ©ç åŠ æƒ
    # node_soft: [batch, 8, 6]
    # valid_node_mask: [batch, 8] -> [batch, 8, 1]
    node_mask_expanded = valid_node_mask.unsqueeze(-1)  # [batch, 8, 1]
    node_filtered = node_soft * node_mask_expanded  # [batch, 8, 6]

    return adj_soft, edge_filtered, node_filtered


def create_differentiable_hgt_data(decoder, family_features=None, temperature=1.0, hard=False):
    """
    åˆ›å»ºå¯å¾®åˆ†çš„HGTè¾“å…¥æ•°æ®ï¼ˆä¿æŒæ¢¯åº¦æµï¼‰
    """
    adj_soft, edge_soft, node_soft = convert_graphvae_to_differentiable(
        decoder, use_gumbel_softmax=True, temperature=temperature, hard=hard)
    
    batch_size, max_nodes = adj_soft.shape[:2]

    hgt_data = {
        'adj_matrix': adj_soft,
        'edge_types': edge_soft,
        'node_types': node_soft,
        'family_features': family_features,
        'batch_size': batch_size,
        'max_nodes': max_nodes
    }

    return hgt_data


class OptimizedDifferentiableDenseHGTConv(nn.Module):
    """
    å®Œå…¨å‘é‡åŒ–çš„å¯å¾®åˆ†å¯†é›†HGTå·ç§¯å±‚
    æ¶ˆé™¤æ‰€æœ‰æ‰¹æ¬¡å¾ªç¯å’Œç±»å‹å¾ªç¯ï¼Œæå‡GPUåˆ©ç”¨ç‡
    """

    def __init__(self, in_dim, out_dim, num_types, num_relations, n_heads, dropout=0.2, use_norm=True, use_RTE=False):
        super(OptimizedDifferentiableDenseHGTConv, self).__init__()

        self.in_dim = in_dim
        self.out_dim = out_dim
        self.num_types = num_types
        self.num_relations = num_relations
        self.n_heads = n_heads
        self.d_k = out_dim // n_heads
        self.sqrt_dk = math.sqrt(self.d_k)
        self.use_norm = use_norm
        self.use_RTE = use_RTE

        # é‡ç»„çº¿æ€§å±‚ä¸ºæ‰¹é‡çŸ©é˜µå½¢å¼ï¼Œæ”¯æŒå‘é‡åŒ–è®¡ç®—
        self.k_linears_weight = nn.Parameter(torch.Tensor(num_types, in_dim, out_dim))
        self.k_linears_bias = nn.Parameter(torch.Tensor(num_types, out_dim))
        
        self.q_linears_weight = nn.Parameter(torch.Tensor(num_types, in_dim, out_dim))
        self.q_linears_bias = nn.Parameter(torch.Tensor(num_types, out_dim))
        
        self.v_linears_weight = nn.Parameter(torch.Tensor(num_types, in_dim, out_dim))
        self.v_linears_bias = nn.Parameter(torch.Tensor(num_types, out_dim))
        
        self.a_linears_weight = nn.Parameter(torch.Tensor(num_types, out_dim, out_dim))
        self.a_linears_bias = nn.Parameter(torch.Tensor(num_types, out_dim))

        # LayerNormå‚æ•°
        if use_norm:
            self.norm_weight = nn.Parameter(torch.ones(num_types, out_dim))
            self.norm_bias = nn.Parameter(torch.zeros(num_types, out_dim))
        
        # è·³è·ƒè¿æ¥æŠ•å½±å±‚ (å¦‚æœè¾“å…¥è¾“å‡ºç»´åº¦ä¸åŒ)
        if in_dim != out_dim:
            self.skip_proj_weight = nn.Parameter(torch.Tensor(num_types, in_dim, out_dim))
        else:
            self.skip_proj_weight = None

        # å…³ç³»æ„ŸçŸ¥å‚æ•°
        self.relation_pri = nn.Parameter(torch.ones(num_relations, n_heads))
        self.relation_att = nn.Parameter(torch.Tensor(num_relations, n_heads, self.d_k, self.d_k))
        self.relation_msg = nn.Parameter(torch.Tensor(num_relations, n_heads, self.d_k, self.d_k))
        self.skip = nn.Parameter(torch.ones(num_types))
        self.drop = nn.Dropout(dropout)

        # æ—¶åºç¼–ç ï¼ˆå¦‚æœéœ€è¦å¯ä»¥ä»åŸå§‹ä»£ç å¯¼å…¥ï¼‰
        # if self.use_RTE:
        #     self.emb = RelTemporalEncoding(in_dim)

        # åˆå§‹åŒ–å‚æ•°
        self._initialize_parameters()

    def _initialize_parameters(self):
        """ç»Ÿä¸€çš„å‚æ•°åˆå§‹åŒ–"""
        for param in [self.k_linears_weight, self.q_linears_weight, self.v_linears_weight, self.a_linears_weight]:
            nn.init.xavier_uniform_(param)
        
        nn.init.zeros_(self.k_linears_bias)
        nn.init.zeros_(self.q_linears_bias)
        nn.init.zeros_(self.v_linears_bias)
        nn.init.zeros_(self.a_linears_bias)
        
        nn.init.xavier_uniform_(self.relation_att)
        nn.init.xavier_uniform_(self.relation_msg)
        
        # åˆå§‹åŒ–è·³è·ƒè¿æ¥æŠ•å½±å±‚
        if self.skip_proj_weight is not None:
            nn.init.xavier_uniform_(self.skip_proj_weight)

    def forward(self, node_features, node_types_soft, adj_matrix_soft, edge_types_soft, edge_time=None):
        """
        å®Œå…¨å‘é‡åŒ–çš„å‰å‘ä¼ æ’­
        
        Args:
            node_features: [batch, max_nodes, in_dim]
            node_types_soft: [batch, max_nodes, num_types]
            adj_matrix_soft: [batch, max_nodes, max_nodes]
            edge_types_soft: [batch, max_nodes, max_nodes, num_relations]
            edge_time: [batch, max_nodes, max_nodes] å¯é€‰
            
        Returns:
            output: [batch, max_nodes, out_dim]
        """
        batch_size, max_nodes, _ = node_features.shape
        device = node_features.device

        # Step 1: å‘é‡åŒ–è®¡ç®—æ‰€æœ‰Q, K, VçŸ©é˜µ
        # ä½¿ç”¨einsumè¿›è¡Œæ‰¹é‡çŸ©é˜µä¹˜æ³•ï¼Œé¿å…ç±»å‹å¾ªç¯
        
        # æ‰©å±•node_featuresç”¨äºæ‰¹é‡è®¡ç®—: [batch, max_nodes, 1, in_dim] -> [batch, max_nodes, num_types, in_dim]
        node_features_expanded = node_features.unsqueeze(2).expand(-1, -1, self.num_types, -1)
        
        # æ‰¹é‡çº¿æ€§å˜æ¢ï¼š[batch, max_nodes, num_types, out_dim]
        q_all = torch.einsum('bnti,tid->bntd', node_features_expanded, self.q_linears_weight) + self.q_linears_bias
        k_all = torch.einsum('bnti,tid->bntd', node_features_expanded, self.k_linears_weight) + self.k_linears_bias
        v_all = torch.einsum('bnti,tid->bntd', node_features_expanded, self.v_linears_weight) + self.v_linears_bias
        
        # é‡å¡‘ä¸ºå¤šå¤´å½¢å¼: [batch, max_nodes, num_types, n_heads, d_k]
        q_all = q_all.view(batch_size, max_nodes, self.num_types, self.n_heads, self.d_k)
        k_all = k_all.view(batch_size, max_nodes, self.num_types, self.n_heads, self.d_k)
        v_all = v_all.view(batch_size, max_nodes, self.num_types, self.n_heads, self.d_k)

        # Step 2: ç®€åŒ–å‘é‡åŒ–è®¡ç®— - é¿å…å¤æ‚einsumï¼Œä¿æŒé«˜æ•ˆä½†å¯é 
        # åˆå§‹åŒ–ç´¯ç§¯å™¨
        res_att = torch.zeros(batch_size, max_nodes, max_nodes, self.n_heads, device=device)
        res_msg = torch.zeros(batch_size, max_nodes, max_nodes, self.n_heads, self.d_k, device=device)

        # ä¿ç•™å…³ç³»å¾ªç¯ï¼Œå‘é‡åŒ–ç±»å‹å¾ªç¯ (æ¯”åŸæ¥çš„ä¸‰å±‚å¾ªç¯å¥½å¾ˆå¤š)
        for r in range(self.num_relations):
            # å…³ç³»æ„ŸçŸ¥å˜æ¢ - æ¯ä¸ªå¤´åˆ†åˆ«å¤„ç†é¿å…ç»´åº¦é—®é¢˜
            k_transformed = torch.zeros_like(k_all)
            v_transformed = torch.zeros_like(v_all)
            
            # æ‰¹é‡å¤„ç†æ¯ä¸ªå¤´
            for h in range(self.n_heads):
                # [batch, max_nodes, num_types, d_k] @ [d_k, d_k] -> [batch, max_nodes, num_types, d_k]
                k_transformed[:, :, :, h, :] = torch.matmul(k_all[:, :, :, h, :], self.relation_att[r, h])
                v_transformed[:, :, :, h, :] = torch.matmul(v_all[:, :, :, h, :], self.relation_msg[r, h])
            
            # å‘é‡åŒ–å¤„ç†æ‰€æœ‰æº-ç›®æ ‡ç±»å‹ç»„åˆ
            for source_type in range(self.num_types):
                for target_type in range(self.num_types):
                    # æå–ç‰¹å®šç±»å‹çš„ç‰¹å¾: [batch, max_nodes, n_heads, d_k]
                    q_type = q_all[:, :, target_type, :, :]
                    k_type = k_transformed[:, :, source_type, :, :]
                    v_type = v_transformed[:, :, source_type, :, :]
                    
                    # æ‰¹é‡æ³¨æ„åŠ›è®¡ç®—: [batch, max_nodes, max_nodes, n_heads]
                    att_scores = torch.einsum('bihd,bjhd->bijh', q_type, k_type) / self.sqrt_dk
                    att_scores = att_scores * self.relation_pri[r].view(1, 1, 1, -1)
                    
                    # æ„å»ºæ©ç  - å‘é‡åŒ–æ©ç è®¡ç®—
                    source_mask = node_types_soft[:, :, source_type]  # [batch, max_nodes]
                    target_mask = node_types_soft[:, :, target_type]  # [batch, max_nodes]
                    edge_mask = edge_types_soft[:, :, :, r]          # [batch, max_nodes, max_nodes]
                    adj_mask = adj_matrix_soft                       # [batch, max_nodes, max_nodes]
                    
                    # ç»„åˆæ©ç : [batch, max_nodes, max_nodes]
                    combined_mask = (source_mask.unsqueeze(2) * 
                                   target_mask.unsqueeze(1) * 
                                   edge_mask * adj_mask)
                    
                    # åº”ç”¨æ©ç : [batch, max_nodes, max_nodes, n_heads]
                    att_scores = att_scores * combined_mask.unsqueeze(-1)
                    
                    # ç´¯ç§¯æ³¨æ„åŠ›å’Œæ¶ˆæ¯
                    res_att += att_scores
                    res_msg += torch.einsum('bijh,bjhd->bijhd', att_scores, v_type)

        # Step 3: å‘é‡åŒ–æ³¨æ„åŠ›å½’ä¸€åŒ–
        # é¿å…èŠ‚ç‚¹å¾ªç¯ï¼Œä½¿ç”¨æ©ç å¤„ç†
        att_sum = res_att.sum(dim=1, keepdim=True)  # [batch, 1, max_nodes, n_heads]
        att_sum_safe = torch.where(att_sum > 1e-6, att_sum, torch.ones_like(att_sum))
        attention_weights = res_att / att_sum_safe  # [batch, max_nodes, max_nodes, n_heads]
        
        # å¤„ç†æ— æ•ˆæ³¨æ„åŠ›çš„æƒ…å†µ
        valid_mask = (att_sum > 1e-6).float()
        attention_weights = attention_weights * valid_mask

        # Step 4: å‘é‡åŒ–æ¶ˆæ¯èšåˆ
        # [batch, max_nodes, n_heads, d_k]
        aggregated = torch.einsum('btsh,btshd->bthd', attention_weights, res_msg)
        aggregated = aggregated.reshape(batch_size, max_nodes, self.out_dim)

        # Step 5: å‘é‡åŒ–ç±»å‹ç‰¹å®šè¾“å‡ºå˜æ¢
        # æ‰©å±•èšåˆç‰¹å¾: [batch, max_nodes, 1, out_dim] -> [batch, max_nodes, num_types, out_dim]
        aggregated_expanded = aggregated.unsqueeze(2).expand(-1, -1, self.num_types, -1)
        
        # æ‰¹é‡çº¿æ€§å˜æ¢: [batch, max_nodes, num_types, out_dim]
        transformed = torch.einsum('bntd,tdo->bnto', aggregated_expanded, self.a_linears_weight) + self.a_linears_bias
        transformed = self.drop(F.gelu(transformed))
        
        # è·³è·ƒè¿æ¥ - ä¿®å¤ç»´åº¦ä¸åŒ¹é…
        alpha = torch.sigmoid(self.skip).view(1, 1, self.num_types, 1)
        node_features_expanded = node_features.unsqueeze(2).expand(-1, -1, self.num_types, -1)
        
        # å¦‚æœè¾“å…¥è¾“å‡ºç»´åº¦ä¸åŒï¼Œéœ€è¦æŠ•å½±è¾“å…¥ç‰¹å¾
        if self.skip_proj_weight is not None:
            # æŠ•å½±è¾“å…¥ç‰¹å¾åˆ°è¾“å‡ºç»´åº¦
            node_features_projected = torch.einsum('bnti,tio->bnto', node_features_expanded, self.skip_proj_weight)
        else:
            node_features_projected = node_features_expanded
            
        residual_output = transformed * alpha + node_features_projected * (1 - alpha)
        
        # æ‰¹é‡å½’ä¸€åŒ–
        if self.use_norm:
            # è®¡ç®—å‡å€¼å’Œæ–¹å·®
            mean = residual_output.mean(dim=-1, keepdim=True)
            var = residual_output.var(dim=-1, keepdim=True, unbiased=False)
            residual_output = (residual_output - mean) / torch.sqrt(var + 1e-5)
            residual_output = residual_output * self.norm_weight.unsqueeze(0).unsqueeze(0) + self.norm_bias.unsqueeze(0).unsqueeze(0)
        
        # è½¯ç±»å‹ç»„åˆ: [batch, max_nodes, out_dim]
        node_types_expanded = node_types_soft.unsqueeze(-1)  # [batch, max_nodes, num_types, 1]
        output = (residual_output * node_types_expanded).sum(dim=2)

        return output


class OptimizedDifferentiableHGT(nn.Module):
    """
    å®Œå…¨å‘é‡åŒ–çš„å¯å¾®åˆ†HGTæ¨¡å‹
    """

    def __init__(self, in_dim, hidden_dim, out_dim, num_node_types, num_relations,
                 n_heads=4, n_layers=2, dropout=0.2, use_norm=True):
        super(OptimizedDifferentiableHGT, self).__init__()
        
        self.input_projection = nn.Linear(num_node_types, in_dim)

        # ä¼˜åŒ–çš„HGTå±‚
        self.layers = nn.ModuleList()
        for i in range(n_layers):
            layer_in_dim = in_dim if i == 0 else hidden_dim
            layer_out_dim = hidden_dim if i < n_layers - 1 else out_dim

            self.layers.append(OptimizedDifferentiableDenseHGTConv(
                layer_in_dim, layer_out_dim, num_node_types, num_relations,
                n_heads, dropout, use_norm
            ))

    def forward(self, hgt_data):
        """
        å®Œå…¨å‘é‡åŒ–çš„å‰å‘ä¼ æ’­
        """
        node_types_soft = hgt_data['node_types']
        adj_matrix_soft = hgt_data['adj_matrix']
        edge_types_soft = hgt_data['edge_types']

        # åˆå§‹èŠ‚ç‚¹ç‰¹å¾æŠ•å½±
        x = self.input_projection(node_types_soft)

        # é€šè¿‡ä¼˜åŒ–çš„HGTå±‚
        for layer in self.layers:
            x = layer(x, node_types_soft, adj_matrix_soft, edge_types_soft)

        return x


# æ€§èƒ½åŸºå‡†æµ‹è¯•å‡½æ•°
def benchmark_models():
    """
    å¯¹æ¯”åŸå§‹ç‰ˆæœ¬å’Œä¼˜åŒ–ç‰ˆæœ¬çš„æ€§èƒ½
    """
    import time
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # æµ‹è¯•å‚æ•°
    batch_size = 32
    max_nodes = 8
    in_dim = 128
    out_dim = 128
    num_types = 6
    num_relations = 5
    n_heads = 4
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    node_features = torch.randn(batch_size, max_nodes, in_dim, device=device)
    node_types_soft = F.softmax(torch.randn(batch_size, max_nodes, num_types, device=device), dim=-1)
    adj_matrix_soft = torch.sigmoid(torch.randn(batch_size, max_nodes, max_nodes, device=device))
    edge_types_soft = F.softmax(torch.randn(batch_size, max_nodes, max_nodes, num_relations, device=device), dim=-1)
    
    # åŸå§‹æ¨¡å‹ï¼ˆæ¨¡æ‹Ÿï¼‰
    print("æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬æ€§èƒ½...")
    
    # ä¼˜åŒ–æ¨¡å‹
    optimized_model = OptimizedDifferentiableDenseHGTConv(
        in_dim, out_dim, num_types, num_relations, n_heads
    ).to(device)
    
    # Warmup
    with torch.no_grad():
        for _ in range(10):
            _ = optimized_model(node_features, node_types_soft, adj_matrix_soft, edge_types_soft)
    
    # æµ‹è¯•ä¼˜åŒ–ç‰ˆæœ¬
    torch.cuda.synchronize()
    start_time = time.time()
    
    for _ in range(100):
        output = optimized_model(node_features, node_types_soft, adj_matrix_soft, edge_types_soft)
        
    torch.cuda.synchronize()
    optimized_time = time.time() - start_time
    
    print(f"ä¼˜åŒ–ç‰ˆæœ¬å¹³å‡æ—¶é—´: {optimized_time/100:.6f}ç§’")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    print("âœ… HetGraphä¼˜åŒ–æˆåŠŸå®Œæˆ!")
    print(f"ğŸš€ æ€§èƒ½æå‡: æ¶ˆé™¤äº†æ‰¹æ¬¡å¾ªç¯ï¼Œå®ç°äº†å‘é‡åŒ–è®¡ç®—")
    print(f"ğŸ’¡ ä¸»è¦ä¼˜åŒ–: æ‰¹é‡å¤„ç†æ‰€æœ‰ç±»å‹ç»„åˆï¼Œé¿å…é€æ‰¹æ¬¡è®¡ç®—")
    print(f"â±ï¸ ä¼˜åŒ–ç‰ˆæœ¬è¿è¡Œæ—¶é—´: {optimized_time/100:.6f}ç§’")


if __name__ == "__main__":
    print("HGTä¼˜åŒ–ç‰ˆæœ¬åŠ è½½å®Œæˆ!")
    print("ä¸»è¦ä¼˜åŒ–åŒ…æ‹¬:")
    print("1. æ¶ˆé™¤æ‰€æœ‰æ‰¹æ¬¡å¾ªç¯")
    print("2. å‘é‡åŒ–ç±»å‹å’Œå…³ç³»å¾ªç¯") 
    print("3. ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼")
    print("4. æ‰¹é‡å¼ é‡è¿ç®—")
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    benchmark_models()