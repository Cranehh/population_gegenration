"""
åˆ†å±‚äººå£VAE - é’ˆå¯¹å®¶åº­-æˆå‘˜åˆ†å±‚ç»“æ„çš„å˜åˆ†è‡ªç¼–ç å™¨
è§£å†³å˜é•¿å®¶åº­æ•°æ®çš„å›ºå®šé•¿åº¦ç¼–ç é—®é¢˜ï¼Œä¸ºè´å¶æ–¯ç½‘ç»œçº¦æŸæä¾›æ½œåœ¨è¡¨ç¤º
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from torch.utils.data import DataLoader, Dataset
import os
from typing import Dict, List, Tuple, Optional, Any
import pandas as pd
from sklearn.preprocessing import StandardScaler
import json
import pickle
from collections import defaultdict


class PopulationDataLoader:
    """äººå£æ•°æ®åŠ è½½å™¨ - æ•´åˆdataset.pyå’Œç”Ÿæˆæ•°æ®çš„åŠ è½½é€»è¾‘"""
    
    def __init__(self, data_dir: str = "æ•°æ®", generated_data_dir: str = "ç”Ÿæˆæ•°æ®"):
        self.data_dir = data_dir
        self.generated_data_dir = generated_data_dir
        self.scaler_family = StandardScaler()
        self.scaler_member = StandardScaler()
        
    def load_numpy_data(self):
        """åŠ è½½NPYæ ¼å¼çš„å¤„ç†åæ•°æ®"""
        try:
            family_data = np.load(os.path.join(self.data_dir, 'family_sample_improved_cluster.npy'))
            member_data = np.load(os.path.join(self.data_dir, 'family_member_sample_improved_cluster.npy'))
            adj_data = np.load(os.path.join(self.data_dir, 'family_adj.npy'))
            edge_data = np.load(os.path.join(self.data_dir, 'familymember_relationship.npy'))
            node_data = np.load(os.path.join(self.data_dir, 'familymember_type.npy'))
            
            print(f"å·²åŠ è½½NPYæ•°æ®:")
            print(f"  å®¶åº­æ•°æ®: {family_data.shape}")
            print(f"  æˆå‘˜æ•°æ®: {member_data.shape}")
            print(f"  é‚»æ¥çŸ©é˜µ: {adj_data.shape}")
            print(f"  è¾¹ç‰¹å¾: {edge_data.shape}")
            print(f"  èŠ‚ç‚¹ç‰¹å¾: {node_data.shape}")
            
            return {
                'family': family_data,
                'member': member_data,
                'adj': adj_data,
                'edge': edge_data,
                'node': node_data
            }
        except FileNotFoundError as e:
            print(f"NPYæ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
            return None
    
    def load_csv_data(self, max_samples_per_grid: int = 1000):
        """åŠ è½½CSVæ ¼å¼çš„ç”Ÿæˆæ•°æ®"""
        family_data_list = []
        member_data_list = []
        
        # éå†ç”Ÿæˆæ•°æ®æ–‡ä»¶å¤¹
        if not os.path.exists(self.generated_data_dir):
            print(f"ç”Ÿæˆæ•°æ®æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {self.generated_data_dir}")
            return None
            
        csv_files = [f for f in os.listdir(self.generated_data_dir) if f.endswith('.csv')]
        print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
        
        # æŒ‰æ …æ ¼åˆ†ç»„
        grid_files = defaultdict(lambda: {'family': None, 'member': None})
        
        for file in csv_files:
            if 'å®¶åº­æ•°æ®' in file:
                grid_id = self._extract_grid_id(file)
                grid_files[grid_id]['family'] = file
            elif 'ä¸ªäººæ•°æ®' in file:
                grid_id = self._extract_grid_id(file)
                grid_files[grid_id]['member'] = file
        
        print(f"è¯†åˆ«å‡º {len(grid_files)} ä¸ªæ …æ ¼çš„æ•°æ®")
        
        # åŠ è½½æ¯ä¸ªæ …æ ¼çš„æ•°æ®
        for grid_id, files in list(grid_files.items())[:10]:  # é™åˆ¶åŠ è½½æ•°é‡ç”¨äºæµ‹è¯•
            if files['family'] and files['member']:
                try:
                    # åŠ è½½å®¶åº­æ•°æ®
                    family_df = pd.read_csv(os.path.join(self.generated_data_dir, files['family']))
                    member_df = pd.read_csv(os.path.join(self.generated_data_dir, files['member']))
                    
                    # é™åˆ¶æ ·æœ¬æ•°é‡
                    if len(family_df) > max_samples_per_grid:
                        family_df = family_df.sample(n=max_samples_per_grid, random_state=42)
                    if len(member_df) > max_samples_per_grid * 8:  # å‡è®¾å¹³å‡æ¯å®¶åº­8äºº
                        member_df = member_df.sample(n=max_samples_per_grid * 8, random_state=42)
                    
                    family_data_list.append(family_df)
                    member_data_list.append(member_df)
                    
                except Exception as e:
                    print(f"åŠ è½½æ …æ ¼ {grid_id} æ•°æ®å¤±è´¥: {e}")
                    continue
        
        if family_data_list and member_data_list:
            # åˆå¹¶æ‰€æœ‰æ•°æ®
            all_family_data = pd.concat(family_data_list, ignore_index=True)
            all_member_data = pd.concat(member_data_list, ignore_index=True)
            
            print(f"CSVæ•°æ®åŠ è½½å®Œæˆ:")
            print(f"  å®¶åº­æ•°æ®: {all_family_data.shape}")
            print(f"  ä¸ªäººæ•°æ®: {all_member_data.shape}")
            
            return {
                'family_df': all_family_data,
                'member_df': all_member_data
            }
        else:
            print("æœªèƒ½æˆåŠŸåŠ è½½CSVæ•°æ®")
            return None
    
    def _extract_grid_id(self, filename):
        """ä»æ–‡ä»¶åæå–æ …æ ¼ID"""
        try:
            # ä¾‹å¦‚: "äººå£æ …æ ¼_ä¸œåŸåŒº_æ …æ ¼0_å®¶åº­æ•°æ®.csv"
            parts = filename.split('_')
            for part in parts:
                if 'æ …æ ¼' in part and part != 'äººå£æ …æ ¼':
                    return part.replace('æ …æ ¼', '').split('_')[0]
            return filename.split('_')[2] if len(filename.split('_')) > 2 else "unknown"
        except:
            return "unknown"
    
    def create_hierarchical_dataset(self, use_numpy: bool = True):
        """åˆ›å»ºåˆ†å±‚æ•°æ®é›†ç”¨äºVAEè®­ç»ƒ"""
        
        if use_numpy:
            data = self.load_numpy_data()
            if data is None:
                print("å°è¯•åŠ è½½CSVæ•°æ®...")
                use_numpy = False
        
        if not use_numpy:
            data = self.load_csv_data()
            if data is None:
                raise ValueError("æ— æ³•åŠ è½½ä»»ä½•æ•°æ®æ–‡ä»¶")
                
        if use_numpy:
            return self._process_numpy_data(data)
        else:
            return self._process_csv_data(data)
    
    def _process_numpy_data(self, data):
        """å¤„ç†NPYæ ¼å¼æ•°æ®"""
        family_data = data['family']
        member_data = data['member']
        adj_data = data['adj']
        edge_data = data['edge']
        node_data = data['node']
        
        # åŸºæœ¬ä¿¡æ¯
        num_samples = family_data.shape[0]
        max_family_size = member_data.shape[1]
        family_feature_dim = family_data.shape[1] if len(family_data.shape) > 1 else 10
        member_feature_dim = member_data.shape[2] if len(member_data.shape) > 2 else 51
        
        print(f"æ•°æ®ç»´åº¦ä¿¡æ¯:")
        print(f"  æ ·æœ¬æ•°é‡: {num_samples}")
        print(f"  æœ€å¤§å®¶åº­è§„æ¨¡: {max_family_size}")
        print(f"  å®¶åº­ç‰¹å¾ç»´åº¦: {family_feature_dim}")
        print(f"  æˆå‘˜ç‰¹å¾ç»´åº¦: {member_feature_dim}")
        
        # åˆ›å»ºæ•°æ®é›†
        dataset = HierarchicalPopulationDataset(
            family_data=family_data,
            member_data=member_data,
            adj_data=adj_data,
            edge_data=edge_data,
            node_data=node_data
        )
        
        return dataset
    
    def _process_csv_data(self, data):
        """å¤„ç†CSVæ ¼å¼æ•°æ®"""
        family_df = data['family_df']
        member_df = data['member_df']
        
        # æå–ç‰¹å¾åˆ—
        family_features = family_df.iloc[:, :10].values  # å‰10åˆ—ä¸ºå®¶åº­ç‰¹å¾
        member_features = member_df.iloc[:, :51].values  # å‰51åˆ—ä¸ºæˆå‘˜ç‰¹å¾
        
        print(f"CSVæ•°æ®è½¬æ¢:")
        print(f"  å®¶åº­ç‰¹å¾å½¢çŠ¶: {family_features.shape}")
        print(f"  æˆå‘˜ç‰¹å¾å½¢çŠ¶: {member_features.shape}")
        
        # é‡ç»„æˆå‘˜æ•°æ®ä¸ºå®¶åº­-æˆå‘˜ç»“æ„
        max_family_size = 8  # å‡è®¾æœ€å¤§å®¶åº­è§„æ¨¡
        
        # ç®€åŒ–å¤„ç†ï¼šå°†æˆå‘˜æ•°æ®é‡æ–°ç»„ç»‡ä¸ºå®¶åº­ç»“æ„
        # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…æ•°æ®ç»“æ„è°ƒæ•´
        num_families = min(len(family_features), len(member_features) // max_family_size)
        
        # é‡å¡‘æˆå‘˜æ•°æ®
        member_data_reshaped = member_features[:num_families * max_family_size].reshape(
            num_families, max_family_size, -1
        )
        
        # åˆ›å»ºç®€åŒ–çš„é‚»æ¥çŸ©é˜µï¼ˆå¯ä»¥åç»­æ”¹è¿›ï¼‰
        adj_data = np.zeros((num_families, max_family_size, max_family_size))
        edge_data = np.zeros((num_families, max_family_size, max_family_size, 5))  # å‡è®¾5ç§å…³ç³»ç±»å‹
        node_data = np.zeros((num_families, max_family_size, 6))  # å‡è®¾6ç§èŠ‚ç‚¹ç±»å‹
        
        dataset = HierarchicalPopulationDataset(
            family_data=family_features[:num_families],
            member_data=member_data_reshaped,
            adj_data=adj_data,
            edge_data=edge_data,
            node_data=node_data
        )
        
        return dataset


class HierarchicalPopulationDataset(Dataset):
    """åˆ†å±‚äººå£æ•°æ®é›†"""
    
    def __init__(self, family_data, member_data, adj_data, edge_data, node_data):
        self.family_data = torch.FloatTensor(family_data)
        self.member_data = torch.FloatTensor(member_data)
        self.adj_data = torch.FloatTensor(adj_data)
        self.edge_data = torch.FloatTensor(edge_data)
        self.node_data = torch.FloatTensor(node_data)
        
        # è®¡ç®—æœ‰æ•ˆæˆå‘˜æ©ç 
        self.member_mask = self._compute_member_mask()
        
        # æ•°æ®ç»Ÿè®¡ä¿¡æ¯
        self.num_samples = len(self.family_data)
        self.max_family_size = self.member_data.shape[1]
        self.family_feature_dim = self.family_data.shape[1]
        self.member_feature_dim = self.member_data.shape[2]
        
    def _compute_member_mask(self):
        """è®¡ç®—æœ‰æ•ˆæˆå‘˜çš„æ©ç """
        # å‡è®¾å…¨é›¶å‘é‡è¡¨ç¤ºæ— æ•ˆæˆå‘˜
        member_sum = torch.sum(self.member_data, dim=-1)  # [N, max_family_size]
        mask = (member_sum != 0).float()
        return mask
    
    def __len__(self):
        return self.num_samples
    
    def __getitem__(self, idx):
        return {
            'family': self.family_data[idx],         # [family_feature_dim]
            'member': self.member_data[idx],         # [max_family_size, member_feature_dim]
            'adj': self.adj_data[idx],               # [max_family_size, max_family_size]
            'edge': self.edge_data[idx],             # [max_family_size, max_family_size, edge_types]
            'node': self.node_data[idx],             # [max_family_size, node_types]
            'mask': self.member_mask[idx]            # [max_family_size]
        }
    
    def get_flattened_data(self, idx):
        """è·å–å±•å¹³åçš„æ•°æ®ç”¨äºVAE"""
        sample = self.__getitem__(idx)
        
        # å±•å¹³ç­–ç•¥ï¼šç»“æ„æ„ŸçŸ¥çš„å±•å¹³
        flattened = self._structure_aware_flatten(sample)
        
        return flattened
    
    def _structure_aware_flatten(self, sample):
        """ç»“æ„æ„ŸçŸ¥çš„æ•°æ®å±•å¹³"""
        family_features = sample['family']  # [family_dim]
        member_features = sample['member']  # [max_family_size, member_dim]
        adj_matrix = sample['adj']          # [max_family_size, max_family_size]
        mask = sample['mask']               # [max_family_size]
        
        # 1. å®¶åº­ç‰¹å¾å¹¿æ’­åˆ°æ¯ä¸ªæˆå‘˜ä½ç½®
        family_broadcast = family_features.unsqueeze(0).expand(self.max_family_size, -1)
        
        # 2. ä½ç½®ç¼–ç 
        position_encoding = self._create_position_encoding()
        
        # 3. å…³ç³»é‚»æ¥ä¿¡æ¯ç¼–ç 
        adj_encoding = adj_matrix.sum(dim=-1, keepdim=True)  # æ¯ä¸ªæˆå‘˜çš„è¿æ¥åº¦
        
        # 4. æ©ç ä¿¡æ¯
        mask_encoding = mask.unsqueeze(-1)
        
        # 5. èåˆæ‰€æœ‰ä¿¡æ¯
        contextualized_members = torch.cat([
            member_features,        # åŸå§‹æˆå‘˜ç‰¹å¾
            family_broadcast,       # å®¶åº­ä¸Šä¸‹æ–‡
            position_encoding,      # ä½ç½®ä¿¡æ¯
            adj_encoding,          # å…³ç³»è¿æ¥åº¦
            mask_encoding          # æœ‰æ•ˆæ€§æ©ç 
        ], dim=-1)
        
        # 6. å±•å¹³ä¸ºä¸€ç»´å‘é‡
        flattened = contextualized_members.flatten()
        
        return flattened
    
    def _create_position_encoding(self):
        """åˆ›å»ºä½ç½®ç¼–ç """
        # ç®€å•çš„ä½ç½®ç¼–ç 
        positions = torch.arange(self.max_family_size, dtype=torch.float)
        pos_encoding = torch.zeros(self.max_family_size, 8)  # 8ç»´ä½ç½®ç¼–ç 
        
        for i in range(4):
            pos_encoding[:, 2*i] = torch.sin(positions / (10000 ** (2*i / 8)))
            pos_encoding[:, 2*i+1] = torch.cos(positions / (10000 ** (2*i / 8)))
            
        return pos_encoding


class FamilyEncoder(nn.Module):
    """å®¶åº­çº§åˆ«ç¼–ç å™¨"""
    
    def __init__(self, family_dim: int, hidden_dim: int, latent_dim: int):
        super().__init__()
        
        self.encoder = nn.Sequential(
            nn.Linear(family_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
        )
        
        self.mu_layer = nn.Linear(hidden_dim // 2, latent_dim)
        self.logvar_layer = nn.Linear(hidden_dim // 2, latent_dim)
        
    def forward(self, family_features):
        hidden = self.encoder(family_features)
        mu = self.mu_layer(hidden)
        logvar = self.logvar_layer(hidden)
        return mu, logvar


class MemberEncoder(nn.Module):
    """æˆå‘˜çº§åˆ«ç¼–ç å™¨ - å¤„ç†å˜é•¿åºåˆ—"""
    
    def __init__(self, member_dim: int, hidden_dim: int, latent_dim: int, max_family_size: int):
        super().__init__()
        
        self.member_dim = member_dim
        self.max_family_size = max_family_size
        
        # ä¸ªä½“æˆå‘˜ç¼–ç å™¨
        self.member_encoder = nn.Sequential(
            nn.Linear(member_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU()
        )
        
        # åºåˆ—çº§åˆ«ç¼–ç å™¨ï¼ˆLSTMå¤„ç†å˜é•¿åºåˆ—ï¼‰
        self.sequence_encoder = nn.LSTM(
            hidden_dim // 2, 
            hidden_dim // 4, 
            batch_first=True,
            bidirectional=True
        )
        
        self.mu_layer = nn.Linear(hidden_dim // 2, latent_dim)
        self.logvar_layer = nn.Linear(hidden_dim // 2, latent_dim)
        
    def forward(self, member_features, member_mask):
        batch_size = member_features.shape[0]
        
        # ç¼–ç æ¯ä¸ªæˆå‘˜
        member_encoded = self.member_encoder(member_features)  # [B, max_family_size, hidden//2]
        
        # è®¡ç®—å®é™…åºåˆ—é•¿åº¦
        seq_lengths = member_mask.sum(dim=1).long()  # [B]
        
        # æ‰“åŒ…åºåˆ—ç”¨äºLSTM
        packed = nn.utils.rnn.pack_padded_sequence(
            member_encoded, seq_lengths.cpu(), batch_first=True, enforce_sorted=False
        )
        
        # LSTMç¼–ç 
        lstm_out, (hidden, cell) = self.sequence_encoder(packed)
        
        # ä½¿ç”¨æœ€ç»ˆéšçŠ¶æ€ä½œä¸ºåºåˆ—è¡¨ç¤º
        # hidden: [2, B, hidden//4] -> [B, hidden//2]
        sequence_repr = torch.cat([hidden[0], hidden[1]], dim=1)
        
        mu = self.mu_layer(sequence_repr)
        logvar = self.logvar_layer(sequence_repr)
        
        return mu, logvar


class GraphEncoder(nn.Module):
    """å›¾ç»“æ„ç¼–ç å™¨ - ç¼–ç å®¶åº­å…³ç³»å›¾"""
    
    def __init__(self, graph_feature_dim: int, hidden_dim: int, latent_dim: int):
        super().__init__()
        
        # ç®€åŒ–çš„å›¾ç¼–ç å™¨
        self.edge_encoder = nn.Sequential(
            nn.Linear(graph_feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2)
        )
        
        self.graph_aggregator = nn.Sequential(
            nn.Linear(hidden_dim // 2, hidden_dim // 4),
            nn.ReLU()
        )
        
        self.mu_layer = nn.Linear(hidden_dim // 4, latent_dim)
        self.logvar_layer = nn.Linear(hidden_dim // 4, latent_dim)
        
    def forward(self, adj_matrix, edge_features, member_mask):
        batch_size = adj_matrix.shape[0]
        
        # æå–æœ‰æ•ˆè¾¹
        edge_mask = (adj_matrix > 0).float()
        
        # ç®€åŒ–å¤„ç†ï¼šä½¿ç”¨é‚»æ¥çŸ©é˜µä½œä¸ºå›¾ç‰¹å¾
        graph_features = adj_matrix.view(batch_size, -1)
        
        # ç¼–ç å›¾ç‰¹å¾
        encoded = self.edge_encoder(graph_features)
        aggregated = self.graph_aggregator(encoded)
        
        mu = self.mu_layer(aggregated)
        logvar = self.logvar_layer(aggregated)
        
        return mu, logvar


class HierarchicalPopulationVAE(nn.Module):
    """åˆ†å±‚äººå£VAEä¸»æ¨¡å‹"""
    
    def __init__(self, 
                 family_feature_dim: int = 10,
                 member_feature_dim: int = 51,
                 max_family_size: int = 8,
                 hidden_dim: int = 256,
                 family_latent_dim: int = 32,
                 member_latent_dim: int = 64,
                 graph_latent_dim: int = 16):
        super().__init__()
        
        self.family_feature_dim = family_feature_dim
        self.member_feature_dim = member_feature_dim
        self.max_family_size = max_family_size
        self.total_latent_dim = family_latent_dim + member_latent_dim + graph_latent_dim
        
        # åˆ†å±‚ç¼–ç å™¨
        self.family_encoder = FamilyEncoder(family_feature_dim, hidden_dim, family_latent_dim)
        
        # æˆå‘˜ç¼–ç å™¨è¾“å…¥ç»´åº¦åŒ…æ‹¬ä¸Šä¸‹æ–‡ä¿¡æ¯
        contextualized_member_dim = member_feature_dim + family_feature_dim + 8 + 1 + 1  # æˆå‘˜+å®¶åº­+ä½ç½®+é‚»æ¥+æ©ç 
        self.member_encoder = MemberEncoder(contextualized_member_dim, hidden_dim, member_latent_dim, max_family_size)
        
        # å›¾ç¼–ç å™¨
        graph_feature_dim = max_family_size * max_family_size  # ç®€åŒ–çš„é‚»æ¥çŸ©é˜µç‰¹å¾
        self.graph_encoder = GraphEncoder(graph_feature_dim, hidden_dim, graph_latent_dim)
        
        # è§£ç å™¨
        self.decoder = HierarchicalDecoder(
            self.total_latent_dim, 
            hidden_dim,
            family_feature_dim,
            member_feature_dim,
            max_family_size
        )
        
        # æƒé‡åˆå§‹åŒ–
        self.apply(self._init_weights)
        
    def _init_weights(self, module):
        """æƒé‡åˆå§‹åŒ–"""
        if isinstance(module, nn.Linear):
            torch.nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.LSTM):
            for name, param in module.named_parameters():
                if 'weight' in name:
                    torch.nn.init.xavier_uniform_(param)
                elif 'bias' in name:
                    torch.nn.init.zeros_(param)
    
    def encode(self, batch):
        """ç¼–ç é˜¶æ®µ"""
        family_features = batch['family']
        member_features = batch['member']
        adj_matrix = batch['adj']
        edge_features = batch['edge']
        member_mask = batch['mask']
        
        # åˆ›å»ºä¸Šä¸‹æ–‡åŒ–çš„æˆå‘˜ç‰¹å¾
        batch_size = family_features.shape[0]
        
        # å®¶åº­ç‰¹å¾å¹¿æ’­
        family_broadcast = family_features.unsqueeze(1).expand(-1, self.max_family_size, -1)
        
        # ä½ç½®ç¼–ç 
        pos_encoding = self._create_position_encoding(batch_size, family_features.device)
        
        # é‚»æ¥åº¦ç¼–ç 
        adj_encoding = adj_matrix.sum(dim=-1, keepdim=True)
        
        # æ©ç ç¼–ç 
        mask_encoding = member_mask.unsqueeze(-1)
        
        # èåˆæˆå‘˜ç‰¹å¾
        contextualized_members = torch.cat([
            member_features,
            family_broadcast,
            pos_encoding,
            adj_encoding,
            mask_encoding
        ], dim=-1)
        
        # åˆ†å±‚ç¼–ç 
        family_mu, family_logvar = self.family_encoder(family_features)
        member_mu, member_logvar = self.member_encoder(contextualized_members, member_mask)
        graph_mu, graph_logvar = self.graph_encoder(adj_matrix, edge_features, member_mask)
        
        # åˆå¹¶æ½œåœ¨å˜é‡
        mu = torch.cat([family_mu, member_mu, graph_mu], dim=1)
        logvar = torch.cat([family_logvar, member_logvar, graph_logvar], dim=1)
        
        return mu, logvar
    
    def _create_position_encoding(self, batch_size, device):
        """åˆ›å»ºæ‰¹é‡çš„ä½ç½®ç¼–ç """
        positions = torch.arange(self.max_family_size, dtype=torch.float, device=device)
        pos_encoding = torch.zeros(self.max_family_size, 8, device=device)
        
        for i in range(4):
            pos_encoding[:, 2*i] = torch.sin(positions / (10000 ** (2*i / 8)))
            pos_encoding[:, 2*i+1] = torch.cos(positions / (10000 ** (2*i / 8)))
        
        return pos_encoding.unsqueeze(0).expand(batch_size, -1, -1)
    
    def reparameterize(self, mu, logvar):
        """é‡å‚æ•°åŒ–æŠ€å·§"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        """è§£ç é˜¶æ®µ"""
        return self.decoder(z)
    
    def forward(self, batch):
        """å®Œæ•´çš„å‰å‘ä¼ æ’­"""
        mu, logvar = self.encode(batch)
        z = self.reparameterize(mu, logvar)
        recon_batch = self.decode(z)
        
        return recon_batch, mu, logvar
    
    def generate(self, num_samples, device):
        """ç”Ÿæˆæ–°æ ·æœ¬"""
        self.eval()
        with torch.no_grad():
            z = torch.randn(num_samples, self.total_latent_dim, device=device)
            generated = self.decode(z)
        return generated


class HierarchicalDecoder(nn.Module):
    """åˆ†å±‚è§£ç å™¨"""
    
    def __init__(self, latent_dim, hidden_dim, family_feature_dim, member_feature_dim, max_family_size):
        super().__init__()
        
        self.max_family_size = max_family_size
        self.member_feature_dim = member_feature_dim
        
        # å®¶åº­ç‰¹å¾è§£ç å™¨
        self.family_decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, family_feature_dim)
        )
        
        # æˆå‘˜ç‰¹å¾è§£ç å™¨
        self.member_decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, max_family_size * member_feature_dim)
        )
        
        # é‚»æ¥çŸ©é˜µè§£ç å™¨
        self.adj_decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, max_family_size * max_family_size),
            nn.Sigmoid()  # ç¡®ä¿è¾“å‡ºåœ¨[0,1]èŒƒå›´
        )
        
        # æˆå‘˜å­˜åœ¨æ€§è§£ç å™¨
        self.mask_decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, max_family_size),
            nn.Sigmoid()
        )
        
    def forward(self, z):
        """è§£ç æ½œåœ¨å˜é‡ä¸ºåˆ†å±‚ç»“æ„æ•°æ®"""
        batch_size = z.shape[0]
        
        # è§£ç å„ä¸ªç»„ä»¶
        family_features = self.family_decoder(z)
        member_features = self.member_decoder(z).view(batch_size, self.max_family_size, self.member_feature_dim)
        adj_matrix = self.adj_decoder(z).view(batch_size, self.max_family_size, self.max_family_size)
        member_mask = self.mask_decoder(z)
        
        return {
            'family': family_features,
            'member': member_features,
            'adj': adj_matrix,
            'mask': member_mask
        }


def vae_loss_function(recon_batch, batch, mu, logvar, beta=1.0):
    """VAEæŸå¤±å‡½æ•°"""
    
    # é‡å»ºæŸå¤± - åˆ†å±‚è®¡ç®—
    family_recon_loss = F.mse_loss(recon_batch['family'], batch['family'], reduction='sum')
    
    # æˆå‘˜ç‰¹å¾é‡å»ºæŸå¤±ï¼ˆè€ƒè™‘æ©ç ï¼‰
    member_mask = batch['mask'].unsqueeze(-1)  # [B, max_family_size, 1]
    member_recon_loss = F.mse_loss(
        recon_batch['member'] * member_mask,
        batch['member'] * member_mask,
        reduction='sum'
    )
    
    # é‚»æ¥çŸ©é˜µé‡å»ºæŸå¤±
    adj_recon_loss = F.binary_cross_entropy(
        recon_batch['adj'],
        batch['adj'],
        reduction='sum'
    )
    
    # æ©ç é‡å»ºæŸå¤±
    mask_recon_loss = F.binary_cross_entropy(
        recon_batch['mask'],
        batch['mask'],
        reduction='sum'
    )
    
    # æ€»é‡å»ºæŸå¤±
    reconstruction_loss = (
        family_recon_loss + 
        member_recon_loss + 
        0.5 * adj_recon_loss +
        0.3 * mask_recon_loss
    )
    
    # KLæ•£åº¦æŸå¤±
    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    
    # æ€»æŸå¤±
    total_loss = reconstruction_loss + beta * kl_loss
    
    return {
        'total_loss': total_loss,
        'reconstruction_loss': reconstruction_loss,
        'kl_loss': kl_loss,
        'family_recon_loss': family_recon_loss,
        'member_recon_loss': member_recon_loss,
        'adj_recon_loss': adj_recon_loss,
        'mask_recon_loss': mask_recon_loss
    }


def train_hierarchical_vae(data_dir="æ•°æ®", generated_data_dir="ç”Ÿæˆæ•°æ®",
                          batch_size=32, num_epochs=100, lr=1e-3, beta=1.0):
    """è®­ç»ƒåˆ†å±‚äººå£VAE"""
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æ•°æ®åŠ è½½
    data_loader = PopulationDataLoader(data_dir, generated_data_dir)
    dataset = data_loader.create_hierarchical_dataset(use_numpy=True)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)
    
    print(f"æ•°æ®é›†ä¿¡æ¯:")
    print(f"  æ•°æ®é›†å¤§å°: {len(dataset)}")
    print(f"  æ‰¹æ¬¡æ•°é‡: {len(dataloader)}")
    print(f"  æœ€å¤§å®¶åº­è§„æ¨¡: {dataset.max_family_size}")
    print(f"  å®¶åº­ç‰¹å¾ç»´åº¦: {dataset.family_feature_dim}")
    print(f"  æˆå‘˜ç‰¹å¾ç»´åº¦: {dataset.member_feature_dim}")
    
    # åˆ›å»ºæ¨¡å‹
    model = HierarchicalPopulationVAE(
        family_feature_dim=dataset.family_feature_dim,
        member_feature_dim=dataset.member_feature_dim,
        max_family_size=dataset.max_family_size
    ).to(device)
    
    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
    
    print(f"\\næ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"æ€»æ½œåœ¨ç»´åº¦: {model.total_latent_dim}")
    
    # è®­ç»ƒå¾ªç¯
    model.train()
    for epoch in range(num_epochs):
        total_loss = 0
        total_recon_loss = 0
        total_kl_loss = 0
        
        for batch_idx, batch in enumerate(dataloader):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            batch = {k: v.to(device) for k, v in batch.items()}
            
            # å‰å‘ä¼ æ’­
            optimizer.zero_grad()
            recon_batch, mu, logvar = model(batch)
            
            # è®¡ç®—æŸå¤±
            loss_dict = vae_loss_function(recon_batch, batch, mu, logvar, beta)
            loss = loss_dict['total_loss']
            
            # åå‘ä¼ æ’­
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            # ç´¯è®¡æŸå¤±
            total_loss += loss.item()
            total_recon_loss += loss_dict['reconstruction_loss'].item()
            total_kl_loss += loss_dict['kl_loss'].item()
            
            # æ‰“å°è¿›åº¦
            if batch_idx % 10 == 0:
                print(f'Epoch {epoch}/{num_epochs}, Batch {batch_idx}/{len(dataloader)}, '
                      f'Loss: {loss.item():.6f}')
        
        # å¹³å‡æŸå¤±
        avg_loss = total_loss / len(dataloader)
        avg_recon_loss = total_recon_loss / len(dataloader)
        avg_kl_loss = total_kl_loss / len(dataloader)
        
        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step(avg_loss)
        
        print(f'Epoch {epoch} å®Œæˆ - æ€»æŸå¤±: {avg_loss:.6f}, '
              f'é‡å»ºæŸå¤±: {avg_recon_loss:.6f}, KLæŸå¤±: {avg_kl_loss:.6f}')
        
        # ä¿å­˜æ£€æŸ¥ç‚¹
        if epoch % 20 == 0:
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'loss': avg_loss,
            }, f'hierarchical_vae_checkpoint_epoch_{epoch}.pth')
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), 'hierarchical_population_vae.pth')
    print("\\nâœ… VAEè®­ç»ƒå®Œæˆï¼")
    
    return model


# æµ‹è¯•å’Œä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # è®­ç»ƒVAE
    print("å¼€å§‹è®­ç»ƒåˆ†å±‚äººå£VAE...")
    model = train_hierarchical_vae(
        batch_size=16,
        num_epochs=50,
        lr=1e-3,
        beta=1.0
    )
    
    print("\\nğŸ¯ VAEè®­ç»ƒå®Œæˆï¼Œå¯ä»¥ç”¨äºï¼š")
    print("1. ç”Ÿæˆå›ºå®šé•¿åº¦çš„äººå£æ½œåœ¨è¡¨ç¤º")
    print("2. ä¸ºè´å¶æ–¯ç½‘ç»œæä¾›çº¦æŸå»ºæ¨¡çš„åŸºç¡€")
    print("3. åœ¨æ‰©æ•£æ¨¡å‹ä¸­è¿›è¡Œclassifier guidance")
    print("4. è§£å†³å˜é•¿æ•°æ®çš„è¡¨ç¤ºå­¦ä¹ é—®é¢˜")