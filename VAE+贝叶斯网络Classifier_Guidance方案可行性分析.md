# VAE+è´å¶æ–¯ç½‘ç»œClassifier Guidanceæ–¹æ¡ˆå¯è¡Œæ€§åˆ†æ

## 1. æ–¹æ¡ˆæ¦‚è¿°

### 1.1 æ ¸å¿ƒæ€æƒ³
```
å˜é•¿å®¶åº­æ•°æ® â†’ å±•å¹³å¤„ç† â†’ VAEç¼–ç  â†’ å›ºå®šé•¿åº¦æ½œåœ¨å˜é‡ â†’ è´å¶æ–¯ç½‘ç»œ â†’ Classifier Guidance
```

**æŠ€æœ¯è·¯çº¿**ï¼š
1. **æ•°æ®å±•å¹³**: å°†å˜é•¿çš„å®¶åº­æˆå‘˜æ•°æ®å±•å¹³ä¸ºå›ºå®šé•¿åº¦å‘é‡
2. **VAEè®­ç»ƒ**: å­¦ä¹ å®¶åº­-æˆå‘˜è”åˆåˆ†å¸ƒçš„æ½œåœ¨è¡¨ç¤º
3. **è´å¶æ–¯å»ºæ¨¡**: åœ¨æ½œåœ¨ç©ºé—´ä¸­å»ºç«‹å±æ€§é—´çš„æ¦‚ç‡ä¾èµ–å…³ç³»
4. **å¼•å¯¼ç”Ÿæˆ**: ä½¿ç”¨è´å¶æ–¯çº¦æŸæŒ‡å¯¼æ‰©æ•£æ¨¡å‹å»å™ªè¿‡ç¨‹

### 1.2 æŠ€æœ¯ä¼˜åŠ¿
- **ç»´åº¦ç»Ÿä¸€**: è§£å†³å˜é•¿æ•°æ®é—®é¢˜ï¼Œä¾¿äºæ‰¹å¤„ç†
- **è¯­ä¹‰ç¼–ç **: VAEæ½œåœ¨ç©ºé—´ä¿ç•™é‡è¦çš„äººå£ç»Ÿè®¡ç‰¹å¾
- **çº¦æŸå»ºæ¨¡**: è´å¶æ–¯ç½‘ç»œåœ¨æ½œåœ¨ç©ºé—´ä¸­å»ºæ¨¡å¤æ‚çº¦æŸå…³ç³»
- **è½¯å¼•å¯¼**: æ¦‚ç‡æ¢¯åº¦å¼•å¯¼è€Œéç¡¬çº¦æŸï¼Œä¿æŒç”Ÿæˆçµæ´»æ€§

## 2. å¯è¡Œæ€§åˆ†æ

### 2.1 æ•°æ®å±‚é¢çš„å¯è¡Œæ€§ âœ…

**ç°æœ‰æ•°æ®åŸºç¡€**ï¼š
- å®¶åº­å±‚ï¼š12ç»´ç‰¹å¾ (å·²æ ‡å‡†åŒ–)
- æˆå‘˜å±‚ï¼š53ç»´Ã—æœ€å¤š8äºº = 424ç»´ (å·²å¤„ç†æ©ç )
- å…³ç³»å›¾ï¼š8Ã—8é‚»æ¥çŸ©é˜µ + è¾¹ç‰¹å¾ + èŠ‚ç‚¹ç‰¹å¾
- èšç±»ä¿¡æ¯ï¼š50ä¸ªå®¶åº­ç±»å‹çš„èšç±»æ ‡ç­¾

**å±•å¹³ç­–ç•¥**ï¼š
```python
# å±•å¹³åçš„ç‰¹å¾å‘é‡ç»“æ„
flattened_vector = [
    family_features,        # 12ç»´å®¶åº­ç‰¹å¾
    member_features_flat,   # 53Ã—8=424ç»´æˆå‘˜ç‰¹å¾ï¼ˆå«maskï¼‰
    adj_matrix_flat,        # 8Ã—8=64ç»´é‚»æ¥å…³ç³»
    edge_features_flat,     # å…³ç³»ç±»å‹ç‰¹å¾
    spatial_features        # æ …æ ¼+è¡Œæ”¿åŒºåŸŸ
]
# æ€»ç»´åº¦ï¼šçº¦500-600ç»´
```

### 2.2 æŠ€æœ¯å±‚é¢çš„å¯è¡Œæ€§ âœ…

**VAEæ¶æ„è®¾è®¡**ï¼š
```python
class HierarchicalPopulationVAE(nn.Module):
    """åˆ†å±‚äººå£VAEï¼Œå¤„ç†å®¶åº­-æˆå‘˜ç»“æ„"""
    
    def __init__(self):
        # åˆ†å±‚ç¼–ç å™¨
        self.family_encoder = FamilyEncoder(family_dim=12)
        self.member_encoder = MemberEncoder(member_dim=53, max_members=8)  
        self.graph_encoder = GraphEncoder()
        
        # æ½œåœ¨ç©ºé—´ç»´åº¦åˆ†é…
        self.family_latent_dim = 32      # å®¶åº­ç‰¹å¾æ½œåœ¨ç©ºé—´
        self.member_latent_dim = 16*8    # æˆå‘˜ç‰¹å¾æ½œåœ¨ç©ºé—´
        self.graph_latent_dim = 16       # å…³ç³»ç»“æ„æ½œåœ¨ç©ºé—´
        
        self.total_latent_dim = 32 + 128 + 16  # 176ç»´å›ºå®šæ½œåœ¨å˜é‡
```

**è´å¶æ–¯ç½‘ç»œåœ¨æ½œåœ¨ç©ºé—´çš„ä¼˜åŠ¿**ï¼š
1. **ç»´åº¦é™ä½**: ä»600ç»´åŸå§‹ç‰¹å¾é™åˆ°176ç»´æ½œåœ¨ç‰¹å¾
2. **è¯­ä¹‰èšé›†**: ç›¸å…³ç‰¹å¾åœ¨æ½œåœ¨ç©ºé—´ä¸­èšé›†ï¼Œæ˜“äºå»ºæ¨¡ä¾èµ–å…³ç³»
3. **å™ªå£°è¿‡æ»¤**: VAEæ»¤é™¤æ— å…³å™ªå£°ï¼Œä¿ç•™æ ¸å¿ƒè¯­ä¹‰ä¿¡æ¯

### 2.3 è®¡ç®—å¤æ‚åº¦å¯è¡Œæ€§ âœ…

**å†…å­˜å ç”¨ä¼°ç®—**ï¼š
```
æ‰¹æ¬¡å¤§å°ï¼š32
åŸå§‹ç»´åº¦ï¼š~600ç»´
æ½œåœ¨ç»´åº¦ï¼š176ç»´
å†…å­˜å ç”¨ï¼š32Ã—176Ã—4å­—èŠ‚ â‰ˆ 22KB (éå¸¸è½»é‡)
```

**è®­ç»ƒæ•ˆç‡**ï¼š
- VAEè®­ç»ƒï¼šç›¸å¯¹ç‹¬ç«‹ï¼Œå¯é¢„å…ˆå®Œæˆ
- è´å¶æ–¯ç½‘ç»œï¼šåœ¨ä½ç»´æ½œåœ¨ç©ºé—´ä¸­è®­ç»ƒï¼Œè®¡ç®—é‡å°
- åœ¨çº¿å¼•å¯¼ï¼šä»…éœ€å‰å‘æ¨ç†ï¼Œå®æ—¶æ€§å¥½

### 2.4 ç†è®ºåŸºç¡€å¯è¡Œæ€§ âœ…

**æ•°å­¦åŸç†**ï¼š
```
P(x) = âˆ« P(x|z)P(z)dz  (VAEç”ŸæˆåŸç†)
P(zâ‚,zâ‚‚,...,zâ‚™) = âˆP(záµ¢|parents(záµ¢))  (è´å¶æ–¯ç½‘ç»œ)
âˆ‡log P(z) = âˆ‡âˆ‘log P(záµ¢|parents(záµ¢))  (æ¦‚ç‡æ¢¯åº¦)
```

**ç†è®ºä¼˜åŠ¿**ï¼š
1. **æ¦‚ç‡ä¸€è‡´æ€§**: VAEå’Œè´å¶æ–¯ç½‘ç»œéƒ½åŸºäºæ¦‚ç‡æ¡†æ¶
2. **å¯å¾®åˆ†æ€§**: æ•´ä¸ªæµç¨‹ç«¯åˆ°ç«¯å¯å¾®åˆ†
3. **æ”¶æ•›ä¿è¯**: åŸºäºå˜åˆ†æ¨ç†çš„ç†è®ºä¿è¯

## 3. æ¶æ„è®¾è®¡

### 3.1 æ•´ä½“æ¶æ„
```
[åŸå§‹å®¶åº­æ•°æ®] 
    â†“ å±•å¹³+é¢„å¤„ç†
[600ç»´ç‰¹å¾å‘é‡]
    â†“ VAEç¼–ç å™¨
[176ç»´æ½œåœ¨å˜é‡z]
    â†“ è´å¶æ–¯ç½‘ç»œ
[çº¦æŸæ¦‚ç‡P(z)]
    â†“ æ¢¯åº¦è®¡ç®—
[å¼•å¯¼ä¿¡å·âˆ‡log P(z)]
    â†“ åº”ç”¨åˆ°æ‰©æ•£é‡‡æ ·
[çº¦æŸæ»¡è¶³çš„æ–°æ ·æœ¬]
```

### 3.2 VAEæ½œåœ¨ç©ºé—´è®¾è®¡
```python
# æ½œåœ¨ç©ºé—´è¯­ä¹‰åˆ†åŒº
latent_space = {
    'family_demographics': z[:32],     # å®¶åº­äººå£ç»Ÿè®¡
    'family_economics': z[32:48],      # å®¶åº­ç»æµç‰¹å¾  
    'member_attributes': z[48:176],    # æˆå‘˜å±æ€§èšåˆ
    'spatial_context': z[176:192],     # ç©ºé—´åœ°ç†ä¿¡æ¯
    'relationship_structure': z[192:]   # å®¶åº­å…³ç³»ç»“æ„
}
```

### 3.3 è´å¶æ–¯ç½‘ç»œç»“æ„
```python
# åœ¨æ½œåœ¨ç©ºé—´ä¸­çš„ä¾èµ–å…³ç³»
bayesian_network = {
    'family_size_z': [],                           # å®¶åº­è§„æ¨¡ï¼ˆæ ¹èŠ‚ç‚¹ï¼‰
    'family_income_z': ['family_size_z'],          # æ”¶å…¥ä¾èµ–è§„æ¨¡
    'education_level_z': ['family_income_z'],       # æ•™è‚²ä¾èµ–æ”¶å…¥
    'occupation_type_z': ['education_level_z'],     # èŒä¸šä¾èµ–æ•™è‚²
    'age_structure_z': ['family_size_z'],          # å¹´é¾„ç»“æ„ä¾èµ–è§„æ¨¡
    'relationship_z': ['family_size_z', 'age_structure_z']  # å…³ç³»ä¾èµ–è§„æ¨¡å’Œå¹´é¾„
}
```

## 4. å®ç°ç­–ç•¥

### 4.1 ä¸‰é˜¶æ®µè®­ç»ƒç­–ç•¥

**é˜¶æ®µ1ï¼šVAEé¢„è®­ç»ƒ**
```python
# ä½¿ç”¨çœŸå®æ•°æ®é¢„è®­ç»ƒVAEï¼Œå­¦ä¹ æ•°æ®åˆ†å¸ƒ
def pretrain_vae():
    for epoch in range(vae_epochs):
        for batch in real_data_loader:
            # å±•å¹³æ•°æ®
            flattened_data = flatten_hierarchical_data(batch)
            
            # VAEå‰å‘ä¼ æ’­
            mu, logvar = encoder(flattened_data)
            z = reparameterize(mu, logvar)
            recon = decoder(z)
            
            # VAEæŸå¤±
            recon_loss = reconstruction_loss(recon, flattened_data)
            kl_loss = kl_divergence(mu, logvar)
            vae_loss = recon_loss + beta * kl_loss
```

**é˜¶æ®µ2ï¼šè´å¶æ–¯ç½‘ç»œè®­ç»ƒ**
```python
# åœ¨VAEæ½œåœ¨ç©ºé—´ä¸­è®­ç»ƒè´å¶æ–¯ç½‘ç»œ
def train_bayesian_network():
    # ç¼–ç æ‰€æœ‰è®­ç»ƒæ•°æ®åˆ°æ½œåœ¨ç©ºé—´
    with torch.no_grad():
        mu, _ = vae.encoder(training_data)
        latent_codes = mu  # ä½¿ç”¨å‡å€¼ä½œä¸ºæ½œåœ¨è¡¨ç¤º
    
    # è®­ç»ƒè´å¶æ–¯åˆ†ç±»å™¨
    bayesian_classifier.fit(latent_codes)
```

**é˜¶æ®µ3ï¼šè”åˆå¾®è°ƒ**
```python
# åœ¨æ‰©æ•£è®­ç»ƒä¸­é›†æˆå¼•å¯¼æœºåˆ¶
def joint_training():
    for batch in diffusion_training:
        # æ ‡å‡†æ‰©æ•£æŸå¤±
        diffusion_loss = compute_diffusion_loss(batch)
        
        # VAEç¼–ç 
        mu, logvar = vae.encoder(batch)
        z = reparameterize(mu, logvar)
        
        # è´å¶æ–¯çº¦æŸ
        constraint_loss = bayesian_classifier(z)
        
        # è”åˆä¼˜åŒ–
        total_loss = diffusion_loss + lambda * constraint_loss
```

### 4.2 åœ¨çº¿å¼•å¯¼é‡‡æ ·
```python
def guided_sampling_step(model, x_t, t, vae, bayesian_classifier):
    with torch.enable_grad():
        x_t = x_t.requires_grad_(True)
        
        # æ ‡å‡†æ‰©æ•£é¢„æµ‹
        noise_pred = model(x_t, t)
        
        # VAEç¼–ç åˆ°æ½œåœ¨ç©ºé—´
        mu, _ = vae.encoder(x_t)
        
        # è´å¶æ–¯çº¦æŸè¯„ä¼°
        constraint_loss = bayesian_classifier.compute_constraint_loss(mu)
        
        # è®¡ç®—æ½œåœ¨ç©ºé—´æ¢¯åº¦
        latent_grad = torch.autograd.grad(constraint_loss, mu)[0]
        
        # é€šè¿‡VAEé›…å¯æ¯”çŸ©é˜µä¼ æ’­æ¢¯åº¦åˆ°åŸç©ºé—´
        input_grad = vae.encoder.backward_gradient(latent_grad, x_t)
        
        # åº”ç”¨å¼•å¯¼
        guided_noise = noise_pred - guidance_scale * input_grad
        
    return guided_noise
```

## 5. æŠ€æœ¯æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ

### 5.1 æŒ‘æˆ˜1ï¼šå±•å¹³æ—¶ä¿¡æ¯ä¸¢å¤±
**é—®é¢˜**ï¼šç®€å•å±•å¹³å¯èƒ½ä¸¢å¤±å±‚çº§ç»“æ„ä¿¡æ¯

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# ç»“æ„æ„ŸçŸ¥çš„å±•å¹³ç­–ç•¥
def structure_aware_flattening(family_data, member_data, adj_matrix):
    # 1. ä¿ç•™ä½ç½®ç¼–ç 
    position_encoding = create_position_encoding(max_family_size=8)
    
    # 2. å…³ç³»æ„ŸçŸ¥ç¼–ç 
    relationship_encoding = encode_relationships(adj_matrix)
    
    # 3. å±‚çº§ç‰¹å¾èåˆ
    family_broadcast = family_data.unsqueeze(1).expand(-1, 8, -1)
    contextualized_members = torch.cat([
        member_data, 
        family_broadcast, 
        position_encoding,
        relationship_encoding
    ], dim=-1)
    
    return contextualized_members.flatten(1)
```

### 5.2 æŒ‘æˆ˜2ï¼šæ½œåœ¨ç©ºé—´è¯­ä¹‰å¯¹é½
**é—®é¢˜**ï¼šVAEæ½œåœ¨å˜é‡ä¸äººå£å±æ€§çš„å¯¹åº”å…³ç³»ä¸æ˜ç¡®

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# Î²-VAE + è¯­ä¹‰æ­£åˆ™åŒ–
class SemanticVAE(nn.Module):
    def __init__(self):
        super().__init__()
        self.beta = 4.0  # å¢å¼ºè§£è€¦æ•ˆæœ
        
    def forward(self, x):
        mu, logvar = self.encoder(x)
        z = self.reparameterize(mu, logvar)
        recon_x = self.decoder(z)
        
        # è¯­ä¹‰å¯¹é½æŸå¤±
        semantic_loss = self.semantic_alignment_loss(z, x)
        
        return recon_x, mu, logvar, semantic_loss
    
    def semantic_alignment_loss(self, z, x):
        # ç¡®ä¿ç‰¹å®šæ½œåœ¨ç»´åº¦å¯¹åº”ç‰¹å®šè¯­ä¹‰
        family_size_pred = self.size_predictor(z[:, :16])
        true_family_size = extract_family_size(x)
        return F.mse_loss(family_size_pred, true_family_size)
```

### 5.3 æŒ‘æˆ˜3ï¼šæ¢¯åº¦ä¼ æ’­ç¨³å®šæ€§
**é—®é¢˜**ï¼šé€šè¿‡VAEçš„æ¢¯åº¦ä¼ æ’­å¯èƒ½ä¸ç¨³å®š

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# ç¨³å®šåŒ–æ¢¯åº¦ä¼ æ’­
def stable_gradient_propagation(vae_encoder, latent_grad, input_x):
    # 1. æ¢¯åº¦è£å‰ª
    latent_grad = torch.clamp(latent_grad, -1.0, 1.0)
    
    # 2. ä½¿ç”¨æ•°å€¼ç¨³å®šçš„é›…å¯æ¯”è¿‘ä¼¼
    with torch.enable_grad():
        input_x_perturb = input_x + torch.randn_like(input_x) * 1e-4
        mu_perturb, _ = vae_encoder(input_x_perturb)
        mu_orig, _ = vae_encoder(input_x)
        
        # æœ‰é™å·®åˆ†è¿‘ä¼¼é›…å¯æ¯”
        jacobian_approx = (mu_perturb - mu_orig) / 1e-4
        
    # 3. ç¨³å®šçš„æ¢¯åº¦ä¼ æ’­
    input_grad = torch.matmul(latent_grad.unsqueeze(1), jacobian_approx).squeeze(1)
    
    return input_grad
```

## 6. é¢„æœŸæ•ˆæœ

### 6.1 è§£å†³é›¶é—®é¢˜ âœ¨
- **æœºåˆ¶**: VAEå­¦ä¹ æ•°æ®çš„è¿ç»­æ½œåœ¨è¡¨ç¤ºï¼Œä¸ºé›¶é¢‘ç‡ç»„åˆåˆ†é…éé›¶æ¦‚ç‡
- **æ•ˆæœ**: å¢åŠ æ ·æœ¬å¤šæ ·æ€§ï¼Œè¦†ç›–æ›´å¤šåˆç†çš„å±æ€§ç»„åˆ

### 6.2 å¤„ç†ç»“æ„é›¶ âœ¨
- **æœºåˆ¶**: è´å¶æ–¯ç½‘ç»œåœ¨æ½œåœ¨ç©ºé—´ä¸­ç¼–ç é€»è¾‘çº¦æŸï¼Œå¼•å¯¼è¿œç¦»ä¸å¯èƒ½ç»„åˆ
- **æ•ˆæœ**: ç”Ÿæˆé€»è¾‘ä¸€è‡´çš„æ ·æœ¬ï¼Œé¿å…ä¸åˆç†ç»„åˆ

### 6.3 ä¿æŒçœŸå®æ€§ âœ¨
- **æœºåˆ¶**: VAEåœ¨çœŸå®æ•°æ®ä¸Šé¢„è®­ç»ƒï¼Œä¿æŒæ•°æ®åˆ†å¸ƒç‰¹å¾
- **æ•ˆæœ**: ç”Ÿæˆæ ·æœ¬ç¬¦åˆçœŸå®äººå£ç»Ÿè®¡è§„å¾‹

### 6.4 æå‡æ•ˆç‡ âœ¨
- **æœºåˆ¶**: åœ¨ä½ç»´æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œçº¦æŸè®¡ç®—ï¼Œé¿å…é«˜ç»´åŸå§‹ç©ºé—´çš„å¤æ‚æ“ä½œ
- **æ•ˆæœ**: å¼•å¯¼è®¡ç®—æ•ˆç‡é«˜ï¼Œé€‚åˆå®æ—¶é‡‡æ ·

## 7. å®éªŒéªŒè¯ç­–ç•¥

### 7.1 VAEé‡å»ºè´¨é‡
```python
# è¯„ä¼°VAEé‡å»ºèƒ½åŠ›
def evaluate_vae_reconstruction():
    # é‡å»ºè¯¯å·®
    recon_mse = F.mse_loss(recon_data, original_data)
    
    # è¯­ä¹‰ä¿æŒåº¦
    semantic_similarity = compute_semantic_similarity(recon_data, original_data)
    
    # åˆ†å±‚ç»“æ„ä¿æŒ
    structure_preservation = evaluate_hierarchical_consistency(recon_data)
```

### 7.2 è´å¶æ–¯çº¦æŸæœ‰æ•ˆæ€§
```python
# è¯„ä¼°çº¦æŸæ»¡è¶³åº¦
def evaluate_constraint_effectiveness():
    # ç»“æ„é›¶æ£€æµ‹ç‡
    structural_zero_rate = detect_structural_zeros(generated_samples)
    
    # è¾¹é™…åˆ†å¸ƒåŒ¹é…åº¦
    marginal_kl_div = compute_marginal_kl_divergence(generated_samples, target_marginals)
    
    # æ¡ä»¶ç‹¬ç«‹æ€§æµ‹è¯•
    independence_score = test_conditional_independence(generated_samples)
```

### 7.3 ç«¯åˆ°ç«¯ç”Ÿæˆè´¨é‡
```python
# ç»¼åˆè¯„ä¼°ç”Ÿæˆè´¨é‡
def evaluate_generation_quality():
    # å¤šæ ·æ€§æŒ‡æ ‡
    diversity_metrics = compute_diversity_metrics(generated_samples)
    
    # çœŸå®æ€§è¯„åˆ†
    realism_score = compute_realism_score(generated_samples, real_samples)
    
    # ç©ºé—´ä¸€è‡´æ€§
    spatial_consistency = evaluate_spatial_constraints(generated_samples)
```

## 8. ç»“è®º

### 8.1 å¯è¡Œæ€§æ€»ç»“ âœ…
- **æ•°æ®åŸºç¡€æ‰å®**: ç°æœ‰æ•°æ®ç»“æ„å®Œå¤‡ï¼Œæ”¯æŒå±•å¹³å’ŒVAEè®­ç»ƒ
- **æŠ€æœ¯æ–¹æ¡ˆæˆç†Ÿ**: VAEå’Œè´å¶æ–¯ç½‘ç»œéƒ½æœ‰æˆç†Ÿçš„ç†è®ºåŸºç¡€å’Œå®ç°
- **è®¡ç®—å¤æ‚åº¦å¯æ§**: åœ¨æ½œåœ¨ç©ºé—´æ“ä½œå¤§å¹…é™ä½è®¡ç®—æˆæœ¬
- **æ•ˆæœé¢„æœŸæ˜ç¡®**: èƒ½å¤ŸåŒæ—¶è§£å†³é›¶é—®é¢˜ã€ç»“æ„é›¶é—®é¢˜å¹¶ä¿æŒç”Ÿæˆè´¨é‡

### 8.2 åˆ›æ–°ä»·å€¼ ğŸš€
1. **æ–¹æ³•åˆ›æ–°**: é¦–æ¬¡å°†VAEæ½œåœ¨ç©ºé—´ä¸è´å¶æ–¯ç½‘ç»œçº¦æŸç»“åˆç”¨äºäººå£åˆæˆ
2. **æ•ˆç‡æå‡**: åœ¨ä½ç»´æ½œåœ¨ç©ºé—´ä¸­è¿›è¡Œçº¦æŸå»ºæ¨¡ï¼Œæ˜¾è‘—æå‡è®¡ç®—æ•ˆç‡
3. **è´¨é‡ä¿è¯**: å¤šå±‚æ¬¡çº¦æŸæœºåˆ¶ç¡®ä¿ç”Ÿæˆæ ·æœ¬çš„é€»è¾‘ä¸€è‡´æ€§å’ŒçœŸå®æ€§

### 8.3 åº”ç”¨å‰æ™¯ ğŸ¯
- **åŸå¸‚è§„åˆ’**: ç”Ÿæˆç¬¦åˆç°å®çº¦æŸçš„äººå£åˆ†å¸ƒæ•°æ®
- **ç¤¾ä¼šç§‘å­¦**: æä¾›é«˜è´¨é‡çš„åˆæˆäººå£æ•°æ®ç”¨äºæ”¿ç­–åˆ†æ
- **æœºå™¨å­¦ä¹ **: ä¸ºä¸‹æ¸¸ä»»åŠ¡æä¾›ä¸°å¯Œã€å¤šæ ·ã€åˆç†çš„è®­ç»ƒæ•°æ®

**ç»¼åˆè¯„ä¼°ï¼šè¯¥æ–¹æ¡ˆå…·æœ‰å¾ˆé«˜çš„å¯è¡Œæ€§å’Œå®ç”¨ä»·å€¼ï¼Œå»ºè®®ç«‹å³å®æ–½ã€‚**